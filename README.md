# üìä Databricks SQL & Spark SQL: Data Intelligence and Analytics Portfolio

This repository showcases practical projects and analytical applications developed using **Databricks SQL** and **Spark SQL in Python**. The projects serve as a culmination of skills acquired from DataCamp's intensive courses: **"Introduction to Databricks SQL"** and **"Introduction to Spark SQL in Python."**

The focus is on applying modern data engineering practices within the **Lakehouse Architecture** and executing advanced business intelligence (BI) analysis on a hyper-scalable platform.

## üåü Key Competencies Demonstrated

* **Databricks SQL & Lakehouse:** Implementing scalable data warehousing solutions and managing data flow through the **Medallion Architecture** (Bronze, Silver, Gold layers) for optimal performance and governance.
* **Spark SQL in PySpark:** Leveraging the distributed computing power of Apache Spark alongside Python for Big Data processing using high-performance, **ANSI SQL-compliant** queries.
* **Advanced SQL Techniques:** Proficient use of **Window Functions** for complex analytical tasks, including time-series analysis (running sums/differences), fast-changing data handling, and Natural Language Processing (NLP) feature engineering.
* **Data Engineering Workflows:** Performing ETL/ELT operations, including complex dataset merging (using `MERGE` statements) and implementing best practices for data storage and updates.
* **Business Intelligence & Visualization:** Creating in-platform visualizations and interactive **Databricks Dashboards** to effectively communicate data stories and business insights.

---

## üìÅ Repository Contents

This repository contains the core notebooks and assets demonstrating the application of these skills:

| File Name | Description | Focus Areas |
| :--- | :--- | :--- |
| **`01.SQL-in-PySpark.ipynb`** | Jupyter notebook illustrating the use of Spark SQL within a PySpark environment. | SQL Table Creation, Advanced Window Functions, Moving Window Analysis for NLP, Caching and Logging Best Practices. |
| **`02.SQL-in-Databricks.dbquery.ipynb`** | Databricks notebook demonstrating a comprehensive data engineering and analysis workflow in Databricks SQL. | Medallion Architecture Implementation, ANSI SQL techniques, Handling High-Velocity Data, Leveraging Platform Differentiators (e.g., `MERGE`). |
| **`IBM Attrition Analysis.lvdash.json`** | Definition file for a Databricks **Liquid Dashboard** created to analyze the IBM Employee Attrition dataset. | Business Intelligence, Corporate Data Storytelling, Interactive Data Visualization. |
| **`Example-Of-Databricks-Dashboard.png`** | A visual screenshot showcasing the completed Databricks Dashboard. | Presentation of Analytical Results, Key Business Metrics Visualization. |

---

## üõ†Ô∏è Technologies & Tools

* **Platform:** Databricks Data Intelligence Platform
* **Data Engine:** Apache Spark (via PySpark)
* **Warehouse Solution:** Databricks SQL
* **Languages:** SQL (ANSI SQL, Spark SQL), Python
* **Visualization:** Databricks Dashboards (Liquid)
* **Key Techniques:** Window Functions, Medallion Architecture, Distributed Computing, ETL/ELT.

---

## üí° How to Run the Notebooks

1.  **Clone** this repository to your local machine.
2.  **Import** the `.ipynb` notebooks into your Databricks Workspace.
3.  **Attach** a running Databricks/Spark cluster to the notebooks.
4.  Ensure you have access to the necessary sample data (or substitute with your own Delta tables).
5.  Execute the cells sequentially to replicate the data engineering and analytical workflows.

---

## üë§ Author

* ** Cem Ozcelik **
* (https://www.linkedin.com/in/cemozcelƒ±k)
* i.cemozcelik@gmail.com
